\documentclass[11pt,a4paper]{article}

% packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{subcaption}

\captionsetup{
    font={it,color=gray},
    labelfont={it,color=gray}
}
\usepackage{booktabs}

\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref} % keep hyperref last


\geometry{margin=2.5cm}

% title
\title{Rock, Paper, Scissors: CNN for Image Classification}
\author{Greta Zehnder}
\date{\today}

\begin{document}

\maketitle

% abstract
\begin{abstract}
    This project aims to investigate the application of Convolutional Neural Networks (CNNs) to the task of image classification using a Rock-Paper-Scissors (RPS) dataset, with the objective of designing, training, and evaluating multiple deep learning models. 

The experimental pipeline is composed of dataset exploration, data preprocessing (which includes train/validation/test splitting, input normalization, and data augmentation), followed by the development of three CNN models (ordered by increasing complexity), and their supervised training and performance evaluation. Finally, a generalization part is carried out to highlight the effectiveness of using CNNs for image classification tasks.

The entire study was carried out in accordance with the official TensorFlow/Keras API documentation.

\end{abstract}

% declaration
\section*{Declaration}
\small\textit{
I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.
}
\normalsize

%table of contents

\tableofcontents
\newpage

% introduction
\section{Introduction}

Convolutional Neural Networks are widely adopted in image classification tasks because they provide a flexible framework for learning visual patterns directly from image data while supporting different architectural and training choices.

In this context, the development of a CNN can be seen as an exploratory process, in which progressively more complex modeling decisions are introduced and evaluated, moving from simple baseline architectures to manually tuned configurations and, eventually, to more systematic hyperparameter search strategies.
Exploring these choices is therefore not only a means of improving performance, but also a way to better understand the sensitivity of a model to different design decisions and training conditions, and to observe how changes in architecture and optimization affect learning dynamics in practice.

This exploratory process is computationally expensive, as training and evaluating multiple configurations requires significant time and resources, which naturally constrains the scope of experimentation and motivates a controlled and incremental evaluation under realistic computational limitations.

%-----------------------------------------------------------------
\section{Data exploration and preprocessing}

\subsection{Exploratory Data Analysis}
The dataset \cite{rockpaperscissors_kaggle} used in this project consists of RGB images representing hand gestures corresponding to the three classes of the Rock-Paper-Scissors game, namely rock, paper, and scissors, and includes a total of 2188 images organized into class-specific subdirectories.
The distribution of images across classes is relatively balanced, with 726 images belonging to the rock class, 712 to paper, and 750 to scissors, a property that is particularly relevant for supervised classification tasks, as it allows model performance across classes to be analyzed without strong bias effects.

All images are stored in PNG format and were acquired under controlled conditions, with a uniform green background and consistent lighting and white balance, which limit environmental variability while still preserving meaningful differences in hand shape, orientation, and gesture configuration.
An analysis of both absolute class frequencies and relative class proportions confirms the near-uniform representation of the three categories, while a visual inspection of randomly sampled images from each class highlights the presence of intra-class variability related to hand positioning and finger articulation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{eda_images/class_distribution.png}
    \hfill
    \includegraphics[width=0.35\textwidth]{eda_images/class_proportions.png}
    \caption{Class distribution (left) and class proportions (right) of the Rock-Paper-Scissors dataset.}
    \label{fig:class_dist}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{eda_images/random_samples.png}
    \caption{Randomly selected sample images from each class of the Rock-Paper-Scissors dataset.}
    \label{fig:random_samples}
\end{figure}

\subsection{Preprocessing}

\subsubsection{Train, validation and test splitting}
The dataset was split into training, validation, and test subsets before model training to enable both hyperparameter tuning and an unbiased final evaluation, with the splitting procedure applied independently to each class using a fixed random seed for reproducibility. In particular, 15\% of the images were assigned to the validation set, 15\% to the test set, and the remaining samples were used for training, preserving the class distribution across all subsets. The resulting data were then organized into a structured directory hierarchy with separate folders for each split and class, to ensure a clear separation between training and evaluation data.

\subsubsection{Normalization}

Image normalization was integrated into the data input pipeline by rescaling pixel values from the original $[0,255]$ range to $[0,1]$ using the \texttt{Rescaling(1./255)} layer provided by the TensorFlow/Keras API~\cite{tensorflow_rescaling}, so that all images were fed to the network on the same numerical scale. This transformation was applied consistently to the training, validation, and test sets, ensuring that the model received comparable inputs during both training and evaluation and helping maintain stable optimization.

The datasets were created using the \texttt{image\_dataset\_from\_directory} utility~\cite{tensorflow_imagedataset,keras_imageloading}, and normalization was implemented through a mapping operation within the \texttt{tf.data} pipeline, which allowed parallel execution and improved data loading efficiency~\cite{tensorflow_parallel_mapping}. In addition, caching and prefetching were used to reduce input latency and keep the data flow efficient during both training and testing~\cite{tensorflow_caching_prefetching}.

\subsubsection{Data augmentation}

Data augmentation was introduced to improve model robustness and reduce overfitting by exposing the network (during training) to slightly modified versions of the input images while preserving the original class label. In this project, augmentation was implemented using Keras image preprocessing layers~\cite{keras_augmentation_layers}, following the workflow recommended in the official TensorFlow documentation~\cite{tensorflow_data_augmentation_examples}: a dedicated augmentation module was defined as a small preprocessing pipeline including random horizontal flipping, small random rotations, and random zoom operations, which are well suited for hand-gesture images as they reflect realistic variations in pose and framing (such as small changes in orientation or distance from the camera) without altering the meaning of the gesture. These transformations were applied only to the training set, whereas the validation and test sets were kept unchanged, so that the evaluation metrics reflect performance on the original data distribution.

The augmentation layers were integrated directly into the model architecture, meaning that they are active during training but automatically disabled during inference, ensuring consistent behavior at deployment while still increasing data variability during learning~\cite{tensorflow_augmentation_inside_model}.

\section{CNN architecture and training}
\subsection{Model A: baseline CNN}

Model A is implemented as a baseline convolutional neural network using the Keras high-level API and the \texttt{Sequential} model structure, which allows layers to be stacked linearly in a transparent and interpretable way \cite{keras_models_overview,keras_sequential_model,keras_sequential_guide}. The purpose of this model is to define a simple yet meaningful reference architecture for the Rock--Paper--Scissors image classification task, so that more advanced models can later be compared against it in a clear, consistent and structured manner.

\subsubsection{Architectural structure}

The architecture of Model A follows a standard convolutional structure commonly used in image classification problems and is summarized below.

\begin{itemize}
    \item \textbf{Input layer}: RGB images with fixed spatial dimensions are provided to the network through an explicit input layer \cite{keras_input_layer}, which clearly defines the expected shape of the data and ensures compatibility with the subsequent layers.
    
    \item \textbf{Data augmentation}: the augmentation pipeline described earlier is applied directly after the input layer (and only during training), introducing small geometric variations that increase data diversity without modifying the class label.
    
    \item \textbf{First convolutional block}: a Conv2D layer with 32 filters and $3\times3$ kernels is used to extract low-level visual patterns (such as edges and simple textures), followed by a MaxPooling2D layer with a $2\times2$ window, which reduces the spatial resolution by downsampling the feature maps and retaining the strongest activations within each local region. ReLU activation functions are applied to introduce non-linearity into the model \cite{keras_conv2d,keras_maxpooling2d,keras_relu}.
    
    \item \textbf{Second convolutional block}: thanks to a second Conv2D layer, the number of filters is increased to 64, allowing the network to learn more complex and abstract representations, and is again followed by a $2\times2$ max-pooling operation that further reduces the spatial dimensions by a factor of two while preserving the most informative features.
    
    \item \textbf{Flatten layer}: the resulting feature maps are then transformed into a one-dimensional vector through a flattening operation, enabling the transition from convolutional feature extraction to fully connected processing \cite{keras_flatten}.
    
    \item \textbf{Fully connected layer}: a dense layer with 128 units and ReLU activation combines the extracted features into a compact internal representation, helping the model integrate information across all spatial locations \cite{keras_dense}.
    
    \item \textbf{Output layer}: the final dense layer contains three neurons with softmax activation, to produce a normalized probability distribution over the three target classes (rock, paper, and scissors).
\end{itemize}

The use of two convolutional blocks allows the network to progressively build hierarchical representations, where earlier layers focus on simple visual structures while deeper layers combine them into more informative and abstract patterns, a behavior that has been widely observed in convolutional architectures and is discussed, for instance, in the VGG network study by Simonyan and Zisserman \cite{simonyan2015vgg}. At the same time, keeping the architecture relatively shallow (i.e., limiting it to two convolutional stages) helps to control model complexity and makes it suitable as a baseline reference for comparison.

\subsubsection{Training configuration}

The model is compiled using the Adam optimizer \cite{keras_optimizers} with a learning rate of 0.001 and the Sparse Categorical Cross-Entropy loss function, which is appropriate for multi-class classification tasks with integer-encoded labels \cite{keras_losses}. Training is performed for 10 epochs using the \texttt{fit} method provided by Keras \cite{keras_model_fit}, while classification accuracy on the validation set is monitored throughout the process in order to observe the learning dynamics and identify possible signs of overfitting.


\subsection{Model B: tuned CNN}

Model B extends the baseline architecture introduced in Model A with the objective of systematically evaluating how a restricted set of architectural and optimization choices affects model performance.  
Rather than proposing a substantially different network design, Model B preserves the overall convolutional structure of the baseline and introduces a controlled tuning phase over depth, capacity, and learning rate.

\subsubsection{Architectural structure}

Model B follows the same general design principles as Model A, while allowing for variable depth and feature extraction capacity.  

\begin{itemize}
    \item \textbf{Input layer}: once again, RGB images of fixed spatial resolution are provided through an explicit input layer.
    
    \item \textbf{Data augmentation}: the same data augmentation pipeline introduced for Model A is applied during training.
    
    \item \textbf{Convolutional blocks}: the feature extraction stage consists of $n\_\text{blocks}$ convolutional blocks.  
    Each block includes:
    \begin{itemize}
        \item a Conv2D layer with $3\times3$ kernels and \texttt{same} padding \cite{keras_conv2d},
        \item a ReLU activation function \cite{keras_relu},
        \item a MaxPooling2D layer for spatial downsampling \cite{keras_maxpooling2d}.
    \end{itemize}
    The number of filters is controlled through a \emph{base} value, \texttt{base\_filters}, and is doubled after each block (i.e., $f, 2f, 4f,\dots$).  
    Therefore, \texttt{base\_filters} determines the initial channel width, while deeper blocks progressively increase representational capacity.

    \item \textbf{Flatten layer}: identically to Model A, the final feature maps are flattened into a one-dimensional representation.
    
    \item \textbf{Fully connected layer}: the classifier head, as before, includes a dense layer, this time with 256 units and ReLU activation, increasing head capacity compared to Model A.
    
    \item \textbf{Output layer}: again, a dense layer with three neurons and softmax activation, to produce class probability estimates.
\end{itemize}

\subsubsection{Hyperparameter tuning}

Model B also introduces an explicit hyperparameter tuning phase based on a grid search strategy, a selection manually conducted and intentionally limited to enable a controlled analysis of the effects of depth, capacity, and optimization dynamics.

The grid search explores combinations of the following hyperparameters:
\begin{itemize}
    \item number of convolutional blocks: $\{2, 3\}$,
    \item base number of convolutional filters: $\{32, 64\}$ (as in Model A),
    \item learning rate of the Adam optimizer: $\{10^{-3}, 3\times10^{-4}\}$.
\end{itemize}

The choice of $\{2,3\}$ convolutional blocks directly builds on Model A, which uses two convolutional blocks: including 2 blocks enables a direct comparison against the baseline depth, while 3 blocks represent a moderate increase in depth to assess whether additional hierarchical feature extraction improves performance.

The learning rate values include the baseline setting used in Model A ($10^{-3}$) and a smaller alternative ($3\times10^{-4}$) to evaluate whether a reduced step size improves optimization stability and generalization \cite{adam_optimizer_lr}.

In total, eight configurations are evaluated under the same data splits and fixed random seed, and, for each configuration, the best validation accuracy observed during training is recorded.  
In case of ties in validation accuracy, the configuration with the lowest validation loss is selected.  
All results are stored in structured JSON and CSV files for reproducibility.

Among the evaluated configurations, the best-performing setup is:
\begin{itemize}
    \item number of convolutional blocks: 3,
    \item base number of filters: 32,
    \item learning rate: $10^{-3}$.
\end{itemize}
This configuration achieved a validation accuracy of 0.9816 and a validation loss of 0.0442, and is therefore selected to train the final instance of Model B.

\subsubsection{Training configuration}

All grid search runs are trained with a fixed dense layer size (256 units) and a maximum of 20 epochs, while early stopping is employed as a regularization strategy by monitoring the validation loss \cite{keras_early_stopping_callback,keras_callbacks_early_stopping}.  
Training is halted if no improvement is observed for three consecutive epochs, and the model weights corresponding to the lowest validation loss are restored.

The final instance of Model B is then retrained using the best hyperparameter configuration identified during tuning, using the Adam optimizer with the selected learning rate, and training dynamics are monitored through validation loss and classification accuracy.


\subsection{Model C: random-search CNN}

Model C represents the most flexible and regularized architecture within the experimental progression: in fact, while Models A and B relied on structured grid exploration, this last model adopts a stochastic hyperparameter optimization strategy based on random search.  
Random search is often more efficient than grid search when only a subset of hyperparameters significantly affects performance, especially in mixed discreteâ€“continuous search spaces.

Compared to Model B, Model C introduces:
\begin{itemize}
    \item an additional convolutional block (up to four),
    \item stronger regularization mechanisms (including L2 regularization \cite{keras_l2_regularizer} and SpatialDropout2D \cite{keras_spatial_dropout2d}),
    \item optional Batch Normalization \cite{keras_batch_normalization} layers to improve training stability,
    \item an improved weight initialization strategy (He normal initialization \cite{he_initialization}),
    \item log-uniform sampling of the learning rate,
    \item learning-rate scheduling during training (via ReduceLROnPlateau \cite{keras_reduce_lr_on_plateau,keras_reduce_lr_on_plateau_tf}).
\end{itemize}


\subsubsection{Architectural structure}

Differently from the previous models, Model C is implemented using the Keras Functional API \cite{keras_functional_api}, to enable a more explicit and flexible definition of the computation graph.

The network consists of $n_{\text{blocks}}$ convolutional blocks, where $n_{\text{blocks}} \in \{3,4\}$, and, as in Model B, the number of filters is controlled by a base value, \texttt{base\_filters}, and doubled after each block (i.e., $f, 2f, 4f, 8f$): this progressive widening increases representational capacity as spatial resolution decreases.

Each convolutional block includes:

\begin{itemize}
    \item \textbf{Conv2D layer}: kernels are initialized using He normal initialization, which is particularly suitable when ReLU activations are employed, as it helps to maintain a stable variance of activations across layers and supports efficient gradient propagation during training. An optional L2 kernel regularizer is also applied, depending on the hyperparameter \texttt{l2\_reg}, introducing a penalty on large weight values in order to reduce overfitting and encourage smoother and more stable solutions.

    \item \textbf{Batch Normalization (optional)}: when enabled, Batch Normalization is applied immediately after convolution.  
    In this case, the convolution is configured with \texttt{use\_bias=False}, choice motivated by the fact that Batch Normalization introduces a learnable shift parameter, rendering the explicit bias term redundant.
    
    \item \textbf{ReLU activation} followed by \textbf{MaxPooling2D} for spatial downsampling, as seen in Model A and Model B.
    
    \item \textbf{SpatialDropout2D (optional)}: applied after pooling to randomly drop entire feature maps, and encouraging robustness in convolutional representations, this mechanism is introduced to strengthen regularization within the feature extraction stage, reducing the risk of overfitting by preventing the network from relying excessively on specific convolutional channels.

\end{itemize}

After the convolutional backbone, feature maps are flattened and passed to a fully connected layer with tunable size (\texttt{dense\_units} $\in$ \{256, 512\}).  
An optional Dropout layer is applied in the classification head (\texttt{dropout\_head}) \cite{keras_dropout}, and the final layer is a softmax classifier over the three output classes.

\subsubsection{Hyperparameter tuning via random search}

Hyperparameters are sampled randomly over the following search space:

\begin{itemize}
    \item number of convolutional blocks: $\{3,4\}$,
    \item base number of filters: $\{32,64\}$,
    \item dense units: $\{256,512\}$,
    \item SpatialDropout2D rate: $\{0.0, 0.10\}$,
    \item dropout rate in the classification head: $\{0.3, 0.4, 0.5\}$,
    \item L2 regularization coefficient: $\{0.0, 10^{-4}\}$,
    \item learning rate: sampled log-uniformly in $[10^{-5}, 10^{-3}]$.
\end{itemize}

Sampling the learning rate from a log-uniform distribution allows exploration across multiple orders of magnitude while maintaining equal relative probability \cite{bergstra_bengio_random_search}.  
Here, the upper bound matches previous models, while the lower bound extends exploration toward smaller step sizes suitable for deeper architectures.

A total of 12 trials are performed, and, for each trial, the best validation accuracy achieved during training is recorded (again, in case of ties in validation accuracy, the configuration with the lowest validation loss is selected).  
As in the previous model, results are saved as JSON and CSV files.

\subsubsection{Training configuration}

Each trial is trained for a maximum of 25 epochs (five more than Model B) using, once again, the Adam optimizer, and early stopping is applied by monitoring validation loss with a patience of 3 epochs. When triggered, the model weights corresponding to the lowest validation loss are restored.

Additionally (as mentioned) Model C incorporates a learning-rate scheduling mechanism via the ReduceLROnPlateau callback, meaning that when validation loss stagnates, the learning rate is reduced multiplicatively, enabling finer convergence during later training stages.

\subsubsection{Best configuration and analysis}

Among the evaluated configurations, the best-performing setup (that corresponds to trial 4) is defined by:

\begin{itemize}
    \item number of convolutional blocks: 4,
    \item base number of filters: 64,
    \item dense units: 256,
    \item SpatialDropout2D rate: 0.1,
    \item dropout rate in the classification head: 0.4,
    \item L2 regularization coefficient: 0.0,
    \item learning rate: $2.85 \times 10^{-5}$,
    \item Batch Normalization: enabled.
\end{itemize}

This configuration achieved a validation accuracy of 0.9847 and a validation loss of 0.0512.

The selected architecture suggests that increased depth (four convolutional blocks) combined with stronger feature capacity (base filters of 64 with progressive doubling) enhances hierarchical feature extraction.  
Interestingly, L2 regularization was not selected, while both SpatialDropout2D and head dropout were retained, indicating that stochastic regularization mechanisms were more effective than weight penalization in this setting.

Moreover, the optimal learning rate ($2.85 \times 10^{-5}$) is significantly lower than those explored in previous models.  
This is consistent with the increased architectural complexity: deeper networks with Batch Normalization and larger representational capacity may benefit from smaller optimization steps to ensure stable convergence and improved generalization.

The final instance of Model C is retrained using this configuration and saved together with the training history and random-search results.


%--------------------------------------------------------------------
\section{Evaluation and comparative analysis}
\subsection{Training and validation curves}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_a/evaluation_a/accuracy_a.png}
        \caption{Model A}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_b/evaluation_b/accuracy_b.png}
        \caption{Model B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_c/evaluation_c/accuracy_c.png}
        \caption{Model C}
    \end{subfigure}
    \caption{Training and validation accuracy curves for the three models.}
    \label{fig:accuracy_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_a/evaluation_a/loss_a.png}
        \caption{Model A}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_b/evaluation_b/loss_b.png}
        \caption{Model B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_c/evaluation_c/loss_c.png}
        \caption{Model C}
    \end{subfigure}
    \caption{Training and validation loss curves for the three models.}
    \label{fig:loss_comparison}
\end{figure}

Figures~\ref{fig:accuracy_comparison} and~\ref{fig:loss_comparison} illustrate the evolution of accuracy and loss during training for all models, highlighting the progressive impact of architectural refinement and optimization strategies.

Model A exhibits smooth and stable convergence: training accuracy increases steadily across epochs, while validation accuracy follows a similar trend with only a small gap between the two curves. The corresponding loss curves decrease consistently, and only a slight rise in validation loss appears in the final epoch, suggesting a mild onset of overfitting. Overall, the baseline architecture demonstrates balanced bias-variance behavior and reliable generalization.

Model B converges faster than the baseline model, with both training and validation accuracy increasing rapidly during the first epochs and reaching high values much earlier than in Model A. At the same time, validation loss decreases quickly and stabilizes at a lower level compared to the baseline. The training and validation curves remain close to each other throughout the process, which suggests that the model does not suffer from significant overfitting.

Early stopping is activated before reaching the maximum number of epochs, indicating that the model had already reached a stable performance level and that further training would not have led to meaningful improvements. Overall, Model B shows a smoother and more efficient learning process, with improved validation performance and no clear signs of underfitting or overfitting.

Model C shows a more irregular behavior in the first training epochs, where validation accuracy and loss fluctuate more than in the previous models. This is likely due to the increased depth of the network and the stronger regularization mechanisms, which make the optimization slightly more unstable at the beginning. After this initial phase, both training and validation curves become smoother and improve steadily over time, with a small and consistent gap between them.

The learning-rate scheduling mechanism helps the model converge more gradually in the later epochs, as visible in the continuous reduction of validation loss. Unlike Model B, early stopping is not activated, since validation loss keeps improving within the fixed number of epochs. Importantly, there are no clear signs of overfitting, as validation performance remains aligned with training performance throughout training. Overall, Model C achieves the highest validation accuracy while maintaining stable generalization behavior.

Taken together, the curves reveal a clear progression: the baseline model provides stable performance, structured hyperparameter tuning improves convergence efficiency and reduces validation loss, and the introduction of random search, deeper architecture, and adaptive learning-rate scheduling yields further, though incrementally smaller, gains in validation accuracy.

\subsection{Test set performance}

\begin{table}[H]
\centering
\caption{Test set performance comparison across models.}
\label{tab:test_results}
\begin{tabular}{lcccc}
\toprule
Model & Test accuracy & Test loss & Macro F1 & Weighted F1 \\
\midrule
Model A & 0.9663 & 0.1267 & 0.9658 & 0.9661 \\
Model B & 0.9693 & 0.1334 & 0.9690 & 0.9693 \\
Model C & 0.9847 & 0.0744 & 0.9845 & 0.9846 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_a/evaluation_a/confusion_matrix_test_a.png}
        \caption{Model A}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_b/evaluation_b/confusion_matrix_test_b.png}
        \caption{Model B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_c/evaluation_c/confusion_matrix_test_c.png}
        \caption{Model C}
    \end{subfigure}
    \caption{Confusion matrices on the test set for all models.}
    \label{fig:cm_comparison}
\end{figure}

Table~\ref{tab:test_results} summarizes the final performance of all models on the held-out test set. 
Model A already achieves strong generalization, with accuracy above 96\%. 
Model B provides a slight improvement in classification performance, although with a marginally higher test loss.

Model C achieves the highest test accuracy (0.9847) and the lowest test loss (0.0744), confirming the effectiveness of deeper architecture, stochastic hyperparameter search, and adaptive learning-rate scheduling.

Figure~\ref{fig:cm_comparison} further highlights this progression: in fact, while Models A and B exhibit minor confusion primarily between paper and rock, Model C shows near-perfect class separation. 
In particular, the classes rock and scissors are classified without error, and only a few paper samples are misclassified.

These results demonstrate that architectural refinement and advanced optimization strategies yield consistent improvements in generalization performance, with Model C providing the most robust, accurate and reliable predictions on unseen data.

\subsection{Error analysis and misclassified examples}

Error analysis is conducted exclusively on the held-out test set to ensure an unbiased assessment of generalization performance.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_a/evaluation_a/misclassified_test_a.png}
        \caption{Model A}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_b/evaluation_b/misclassified_test_b.png}
        \caption{Model B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../models/model_c/evaluation_c/misclassified_test_c.png}
        \caption{Model C}
    \end{subfigure}
    \caption{Examples of misclassified test samples for each model. True labels are shown above each image and predicted labels below.}
    \label{fig:misclassified_examples}
\end{figure}

\paragraph{Model A.}

Figure~\ref{fig:misclassified_examples} (Model A) displays up to nine representative misclassified test samples. 
As we have observed in the confusion matrix, Model A misclassifies seven instances of \textit{paper} and four instances of \textit{rock}, while all \textit{scissors} samples are correctly classified.
The examples plotted show how errors involve the class \textit{paper}, which is sometimes predicted as \textit{scissors} when the fingers are partially separated, or as \textit{rock} when the hand is slightly rotated or partially closed. 
The misclassified \textit{rock} samples are instead predicted as \textit{paper}, typically in cases where lighting or hand orientation reduces the visual prominence of the closed fist.

\paragraph{Model B.}

Figure~\ref{fig:misclassified_examples} (Model B) shows nine (of the ten total errors) representative misclassified test samples. 
Although the overall number of errors is comparable to Model~A, their distribution changes slightly.

Several misclassifications still involve the class \textit{paper}, which is often predicted as \textit{rock} and occasionally as \textit{scissors}, confirming that flat-hand configurations remain visually ambiguous under certain orientations. 
In addition, two \textit{rock} samples are predicted as \textit{paper}, and one \textit{scissors} sample is also classified as \textit{paper}. 

Compared to Model A, the confusion appears more evenly distributed across classes rather than concentrated primarily on \textit{paper}. 

\paragraph{Model C.}

Figure~\ref{fig:misclassified_examples} (Model C) shows a smaller and more concentrated set of errors compared to the previous models: all misclassified samples belong to the class \textit{paper}, which is occasionally predicted as either \textit{rock} or \textit{scissors}, while no instances of \textit{rock} or \textit{scissors} are incorrectly classified. 

This suggests that the model has learned to clearly distinguish between closed-fist and two-finger configurations, and that the remaining ambiguity is limited to certain flat-hand gestures where finger spread, orientation, or lighting conditions make the visual pattern less distinct. 

These observations further confirm that, as the qualitative inspection of the errors aligns with the test metrics, Model C achieves the most stable and reliable behavior among the three architectures.


\subsection{Discussion}
While Model C achieves the highest generalization performance, these improvements come at the cost of an increased computational complexity, as deeper architectures, stochastic hyperparameter search, and adaptive learning-rate scheduling significantly raise training time and resource consumption compared to the baseline model. The addition of further convolutional blocks and regularization mechanisms increases both the number of trainable parameters and the overall optimization burden, making the model less suitable for constrained environments despite its superior accuracy.

Although further gains might be obtained through broader hyperparameter exploration, more diverse data augmentation, or transfer learning from pre-trained backbones, such strategies were intentionally excluded to preserve a controlled experimental design and manageable computational cost. The results therefore underline a practical trade-off between architectural sophistication and efficiency, suggesting Model B as a balanced compromise under limited resources, while Model C remains preferable when maximizing predictive performance is the primary objective.

%----------------------------------------------------------
%optional generalization test
\section{ Generalization test}

A final generalization test was conducted on a small external dataset composed of 15 images (5 per class), acquired under different conditions with respect to the training data.

Both Model A and Model B completely failed to generalize, predicting the class \textit{paper} for all input images, regardless of the true label. This behavior indicates a strong bias toward a single class when exposed to out-of-distribution data.

Model C showed a slightly more differentiated behavior. While all \textit{paper} samples were correctly classified, \textit{rock} and \textit{scissors} images were frequently misclassified, with a tendency to be predicted as \textit{paper}. Although Model C did not achieve satisfactory performance, it was the only model that did not collapse entirely to a single-class prediction.

This experiment highlights a key limitation of the project: despite high in-distribution accuracy, the models remain highly sensitive to domain shifts. The results suggest that background characteristics and acquisition conditions may have significantly influenced the learned representations. Increasing dataset diversity and improving robustness to varying visual contexts would likely enhance real-world generalization performance.

\section{Conclusion}

This work explored the development of Convolutional Neural Networks through a progressive and structured modeling strategy, moving from a simple baseline architecture (Model A) to more refined configurations (Models B and C). While Model A demonstrated that a shallow network can effectively capture the core visual patterns of the dataset, subsequent models introduced controlled increases in depth, regularization, and hyperparameter optimization.

The results show that architectural and optimization choices influence learning behavior and in-distribution performance, with Model C achieving the highest validation accuracy. However, performance improvements became progressively smaller as model complexity increased, revealing diminishing returns relative to computational cost.

The external generalization test further highlighted a key limitation: despite strong in-distribution accuracy, all models remained sensitive to domain shifts. This confirms that high validation performance does not necessarily translate into robust real-world generalization.

Overall, the study emphasizes the importance of systematic experimentation, balanced model design, and careful evaluation when developing CNN-based image classification systems.



%bibliography
\bibliographystyle{plain}
\bibliography{references}

\end{document}
