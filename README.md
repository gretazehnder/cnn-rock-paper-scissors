# Rockâ€“Paperâ€“Scissors Image Classification with CNNs

## Project Overview

This project investigates the application of Convolutional Neural Networks (CNNs) to a multi-class image classification task based on the Rockâ€“Paperâ€“Scissors (RPS) dataset.

The objective is to progressively design, train, and evaluate increasingly complex architectures in order to analyze:

- the impact of architectural depth,
- the role of hyperparameter tuning strategies,
- the effect of regularization mechanisms,
- and the trade-off between predictive performance and computational cost.

Three models are developed and compared, following a structured experimental progression from a simple baseline to a deeper architecture optimized through stochastic search.

---
## ğŸ“ Repository Structure

```text
cnn-rock-paper-scissors/
â”‚
â”œâ”€â”€ dataset/
â”œâ”€â”€ dataset_splits/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_a/
â”‚   â”‚   â”œâ”€â”€ evaluation_a/
â”‚   â”‚   â”œâ”€â”€ history_a.json
â”‚   â”‚   â”œâ”€â”€ train_val_a.py
â”‚   â”‚   â””â”€â”€ test_a.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model_b/
â”‚   â”‚   â”œâ”€â”€ evaluation_b/
â”‚   â”‚   â”œâ”€â”€ history_b.json
â”‚   â”‚   â”œâ”€â”€ train_val_b.py
â”‚   â”‚   â””â”€â”€ test_b.py
â”‚   â”‚
â”‚   â””â”€â”€ model_c/
â”‚       â”œâ”€â”€ evaluation_c/
â”‚       â”œâ”€â”€ history_c.json
â”‚       â”œâ”€â”€ train_val_c.py
â”‚       â””â”€â”€ test_c.py
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ data_pipeline.py
â”‚   â””â”€â”€ data_split.py
â”‚
â”œâ”€â”€ report-latex/
â”œâ”€â”€ data_exploration.ipynb
â”œâ”€â”€ data_setup.py
â””â”€â”€ generalization_test.py
```
### `dataset/`
Contains the original Rockâ€“Paperâ€“Scissors images organized by class.

### `dataset_splits/`
Stores the train, validation, and test splits generated with a fixed random seed to ensure reproducibility.

### `preprocessing/`
Includes:
- `data_split.py`: script for structured train/validation/test splitting  
- `data_pipeline.py`: TensorFlow input pipeline (loading, normalization, augmentation, caching, prefetching)

### `models/`
Each model has its own folder (`model_a`, `model_b`, `model_c`) containing:

- `train_val_x.py`: training script  
- `test_x.py`: final test evaluation script  
- `history_x.json`: saved training history  
- `evaluation_x/`: evaluation artifacts, including:
  - training and validation curves  
  - classification reports  
  - confusion matrices  
  - test metrics  
  - misclassified sample visualizations  

Saved `.keras` model files are excluded from version control via `.gitignore` and can be regenerated by re-running the corresponding training scripts.

### `report-latex/`
Contains the full academic report describing methodology, architecture design, experiments, and results.

### `data_exploration.ipynb`
Exploratory Data Analysis notebook used for dataset inspection and visualization.

### `data_setup.py`
Utility script for dataset preparation and project setup.

### `generalization_test.py`
Script used to evaluate model robustness on a small external dataset under domain shift.
---

##  Model Overview

###  Model A â€” Baseline CNN

A shallow convolutional architecture designed as a reference model.

- Two convolutional blocks  
- Standard ReLU activations and max pooling  
- Single dense classification head  
- Fixed training configuration  

Provides stable convergence and strong baseline performance.

---

###  Model B â€” Tuned CNN (Grid Search)

Extends the baseline architecture by introducing controlled hyperparameter tuning.

- Variable number of convolutional blocks  
- Variable base number of filters  
- Learning rate selection via grid search  
- Early stopping for regularization  

Designed to evaluate how moderate increases in depth and capacity affect performance.

---

###  Model C â€” Random-Search CNN

The most flexible and regularized architecture in the project.

Introduces:

- Deeper convolutional backbone  
- Batch Normalization  
- SpatialDropout2D  
- Dropout in the classification head  
- Optional L2 regularization  
- He normal initialization  
- Log-uniform learning rate sampling  
- ReduceLROnPlateau scheduling  

Hyperparameters are selected through random search across a mixed discreteâ€“continuous space.

This model achieves the highest predictive performance while increasing computational complexity.

---

The project emphasizes systematic experimentation, structured model comparison, and transparent evaluation across progressively refined CNN architectures.
